{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://dev.to/amit_kharel_aae65abe2b111/heres-how-you-can-build-and-train-gpt-2-from-scratch-using-pytorch-345n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "89"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dir = \"data.txt\"\n",
    "text = open(data_dir, 'r', encoding='utf8').read() # load all the data as simple string\n",
    "\n",
    "# Get all unique characters in the text as vocabulary\n",
    "chars = list(set(text))\n",
    "vocab_size = len(chars)\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the character level tokenizer\n",
    "chr_to_idx = {c:i for i, c in enumerate(chars)}\n",
    "idx_to_chr = {i:c for i, c in enumerate(chars)}\n",
    "\n",
    "def encode(input_text: str) -> list[int]:\n",
    "    return [chr_to_idx[t] for t in input_text]\n",
    "\n",
    "def decode(input_tokens: list[int]) -> str:\n",
    "    return \"\".join([idx_to_chr[i] for i in input_tokens])\n",
    "\n",
    "\n",
    "import torch\n",
    "\n",
    "# use cpu or gpu based on your system\n",
    "device = \"gpu\"\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "\n",
    "# convert our text data into tokenized tensor\n",
    "data = torch.tensor(encode(text), device=device) #dtyppe=torch.long, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batch_size = 16  # training batch size\n",
    "eval_batch_size = 8  # evaluation batch size\n",
    "context_length = 256  # number of tokens processed in a single batch\n",
    "train_split = 0.8  # percentage of data to use from total data for training\n",
    "\n",
    "# split data into trian and eval\n",
    "n_data = len(data)\n",
    "train_data = data[:int(n_data * train_split)]\n",
    "eval_data = data[int(n_data * train_split):]\n",
    "\n",
    "\n",
    "class DataLoader:\n",
    "    def __init__(self, tokens, batch_size, context_length) -> None:\n",
    "        self.tokens = tokens\n",
    "        self.batch_size = batch_size\n",
    "        self.context_length = context_length\n",
    "\n",
    "        self.current_position = 0\n",
    "\n",
    "    def get_batch(self) -> torch.tensor:\n",
    "        b, c = self.batch_size, self.context_length\n",
    "\n",
    "        start_pos = self.current_position\n",
    "        end_pos = self.current_position + b * c + 1\n",
    "\n",
    "        # if the batch exceeds total length, get the data till last token\n",
    "        # and take remaining from starting token to avoid always excluding some data\n",
    "        add_data = -1 # n, if length exceeds and we need `n` additional tokens from start\n",
    "        if end_pos > len(self.tokens):\n",
    "            add_data = end_pos - len(self.tokens) - 1\n",
    "            end_pos = len(self.tokens) - 1\n",
    "\n",
    "        d = self.tokens[start_pos:end_pos]\n",
    "        if add_data != -1:\n",
    "            d = torch.cat([d, self.tokens[:add_data]])\n",
    "        x = (d[:-1]).view(b, c)  # inputs\n",
    "        y = (d[1:]).view(b, c)  # targets\n",
    "\n",
    "        self.current_position += b * c # set the next position\n",
    "        return x, y\n",
    "\n",
    "train_loader = DataLoader(train_data, train_batch_size, context_length)\n",
    "eval_loader = DataLoader(eval_data, eval_batch_size, context_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# used to define size of embeddings\n",
    "d_model = vocab_size \n",
    "\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class GPT(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model):\n",
    "        super().__init__()\n",
    "        self.wte = nn.Embedding(vocab_size, d_model) # word token embeddings\n",
    "\n",
    "    def forward(self, inputs, targets = None):\n",
    "        logits = self.wte(inputs) # dim -> batch_size, sequence_length, d_model\n",
    "        loss = None\n",
    "        if targets != None:\n",
    "            batch_size, sequence_length, d_model = logits.shape\n",
    "            # to calculate loss for all token embeddings in a batch\n",
    "            # kind of a requirement for cross_entropy\n",
    "            logits = logits.view(batch_size * sequence_length, d_model)\n",
    "            targets = targets.view(batch_size * sequence_length)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, inputs, max_new_tokens):\n",
    "        # this will store the model outputs along with the initial input sequence\n",
    "        # make a copy so that it doesn't interfare with model \n",
    "        for _ in range(max_new_tokens):\n",
    "            # we only pass targets on training to calculate loss\n",
    "            logits, _ = self(inputs)  \n",
    "            # for all the batches, get the embeds for last predicted sequence\n",
    "            logits = logits[:, -1, :] \n",
    "            probs = F.softmax(logits, dim=1)            \n",
    "            # get the probable token based on the input probs\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) \n",
    "\n",
    "            inputs = torch.cat([inputs, idx_next], dim=1)\n",
    "        # as the inputs has all model outputs + initial inputs, we can use it as final output\n",
    "        return inputs\n",
    "\n",
    "m = GPT(vocab_size=vocab_size, d_model=d_model).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([81, 46, 55,  ...,  0, 84, 70], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    input = torch.tensor(encode('cancer is'), dtype=torch.long, device=device).unsqueeze(0)\n",
    "    print(m.generate(input,max_new_tokens=1000)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "shape '[16, 256]' is invalid for input of size 4094",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 8\u001b[0m\n\u001b[0;32m      6\u001b[0m eval_steps \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1000\u001b[39m \u001b[38;5;66;03m# perform evaluation in every n steps\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m ep \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[1;32m----> 8\u001b[0m     xb, yb \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_loader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m     logits, loss \u001b[38;5;241m=\u001b[39m m(xb, yb)\n\u001b[0;32m     11\u001b[0m     optim\u001b[38;5;241m.\u001b[39mzero_grad(set_to_none\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "Cell \u001b[1;32mIn[13], line 36\u001b[0m, in \u001b[0;36mDataLoader.get_batch\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m add_data \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m     35\u001b[0m     d \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([d, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokens[:add_data]])\n\u001b[1;32m---> 36\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[43m(\u001b[49m\u001b[43md\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# inputs\u001b[39;00m\n\u001b[0;32m     37\u001b[0m y \u001b[38;5;241m=\u001b[39m (d[\u001b[38;5;241m1\u001b[39m:])\u001b[38;5;241m.\u001b[39mview(b, c)  \u001b[38;5;66;03m# targets\u001b[39;00m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrent_position \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m b \u001b[38;5;241m*\u001b[39m c \u001b[38;5;66;03m# set the next position\u001b[39;00m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: shape '[16, 256]' is invalid for input of size 4094"
     ]
    }
   ],
   "source": [
    "lr = 1e-3\n",
    "optim = torch.optim.AdamW(m.parameters(), lr=lr)\n",
    "\n",
    "\n",
    "epochs = 5000\n",
    "eval_steps = 1000 # perform evaluation in every n steps\n",
    "for ep in range(epochs):\n",
    "    xb, yb = train_loader.get_batch()\n",
    "\n",
    "    logits, loss = m(xb, yb)\n",
    "    optim.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optim.step()\n",
    "\n",
    "    if ep % eval_steps == 0 or ep == epochs-1:\n",
    "        m.eval()\n",
    "        with torch.no_grad():\n",
    "            xvb, yvb = eval_loader.get_batch()\n",
    "            _, e_loss = m(xvb, yvb)\n",
    "\n",
    "            print(f\"Epoch: {ep}\\tlr: {lr}\\ttrain_loss: {loss}\\teval_loss: {e_loss}\")\n",
    "        m.train() # back to training mode"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
